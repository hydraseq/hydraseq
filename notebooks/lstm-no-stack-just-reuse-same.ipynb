{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sun Feb 10 4:44 am\n",
    "* Initial pass at an LSTM based on hydraseq\n",
    "* It has context, which builds a hirarchical tree of convos at high level\n",
    "* Still need to define state a little better with active/predicted\n",
    "* Still need to use context, add to it so we can handle she/it etc from one sentence to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niarfe/tmprepos/hydra_inc/hydraseq/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "import sys\n",
    "sys.path.append(\"/Users/niarfe/tmprepos/hydra_inc/hydraseq\")\n",
    "import hydraseq\n",
    "from hydraseq import Hydraseq\n",
    "from hydraseq.columns import *\n",
    "! pip list | grep hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0_NOU], [1_VP], [2_SENT]], [[], [], []]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[0, 1, ['0_ADJ', '0_NOU', '0_VER']],\n",
       "  [1, 2, ['0_NOU', '0_VER']],\n",
       "  [2, 3, ['0_ADJ', '0_NOU', '0_VER']]],\n",
       " [[0, 1, ['1_NP', '1_VP']],\n",
       "  [0, 2, ['1_NP', '1_VP']],\n",
       "  [1, 2, ['1_NP', '1_VP']],\n",
       "  [1, 3, ['1_VP']],\n",
       "  [2, 3, ['1_NP', '1_VP']]],\n",
       " [[0, 2, ['2_SENT']],\n",
       "  [1, 3, ['2_SENT']],\n",
       "  [2, 4, ['2_SENT']],\n",
       "  [3, 5, ['2_SENT']]]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"the quick brown fox jumped over the lazy dog\"\n",
    "sentence = \"spring leaves spring\"\n",
    "source_file = [\"seasons.0.txt\", \"seasons.1.txt\", \"seasons.2.txt\"]\n",
    "\n",
    "\n",
    "class MiniColumn:\n",
    "    \"\"\"A stack of trained hydras which can get layers of convolutions\n",
    "    Initialize this with a set of training files, one per hydra.\n",
    "    \n",
    "        run_convolutions: Insert a sentence and get back the stack of convolutions\n",
    "        get_state: Returns the collective state of active and predicted nodes in hydras\n",
    "    \"\"\"\n",
    "    def __init__(self, source_files=[], dir_root='.'):\n",
    "        \"\"\"Initialize hydras from files.\n",
    "        Args\n",
    "            source_files: list<str> a list of filenames with name formated in triplets.\n",
    "                                filename.uuid.ext, uuid should be the internal end marker\n",
    "            dir_root: str, a directory base if the files are not located in script dir\n",
    "        Returns\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.base_hydra = hydraseq.Hydraseq('_')\n",
    "        self.hydras = []\n",
    "        for fname in source_files:\n",
    "            base, uuid, ext = fname.split('.')\n",
    "            h = hydraseq.Hydraseq(uuid+'_')\n",
    "            with open(\"{}/{}\".format(dir_root, fname), 'r') as source:\n",
    "                for line in source:\n",
    "                    h.insert(line.strip())\n",
    "            self.hydras.append(h)\n",
    "        self.depth = len(self.hydras)\n",
    "        self.convolutions = []\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"reset all hydras, and set convolutions, active and predicted arrays to empty\"\"\"\n",
    "        [hydra.reset() for hydra in self.hydras]\n",
    "        self.convolutions = []\n",
    "        self.active = []\n",
    "        self.predicted = []\n",
    "        \n",
    "    def run_convolutions(self, sentence):\n",
    "        \"\"\"Generate the stack of convolutions using this sentence\n",
    "        Internally calculates the convolution and saves them in self.convolutions.\n",
    "        Each convolution is then forward fed to the next hydra.\n",
    "        \n",
    "        Args:\n",
    "            sentence: str, A sentence in plain separated words\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        convos = self.hydras[0].convolutions(sentence)\n",
    "        self.convolutions.append(convos)\n",
    "        for hydra in self.hydras[1:]:\n",
    "            convos = run_convolutions(patterns_only(convos), hydra, hydra.uuid)\n",
    "            self.convolutions.append(convos)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return the states of the internal hydras\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            list<list<active nodes>, list<next nodes>>\n",
    "        \"\"\"\n",
    "        self.active = []\n",
    "        self.predicted = []\n",
    "        for hydra in self.hydras:\n",
    "            self.active.append(hydra.active_nodes)\n",
    "            self.predicted.append(hydra.next_nodes)\n",
    "        return [self.active, self.predicted]\n",
    "            \n",
    "\n",
    "lstm = MiniColumn(source_file)\n",
    "print(lstm.get_state())\n",
    "lstm.run_convolutions(sentence).convolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2:\n",
    "    def __init__(self, datasource=None):\n",
    "        self.hdqs = [\n",
    "            hydraseq.Hydraseq('_'),\n",
    "            hydraseq.Hydraseq('0_'),\n",
    "            hydraseq.Hydraseq('1_'),\n",
    "            hydraseq.Hydraseq('2_')\n",
    "        ]\n",
    "            \n",
    "        self.sentence = \"\"\n",
    "        self.convos0 = []\n",
    "        self.convos1 = []\n",
    "        self.convos2 = []\n",
    "        if datasource: self.consume_data(datasource)\n",
    "        self.sdrs = [\n",
    "            self.sentence,\n",
    "            self.convos0,\n",
    "            self.convos1,\n",
    "            self.convos2\n",
    "        ]\n",
    "        self.context = []\n",
    "        \n",
    "    \n",
    "    def consume_data(self, fpaths):\n",
    "        for fpath, hdq in zip(fpaths, self.hdqs[1:]):\n",
    "            with open(fpath, 'r') as source:\n",
    "                [hdq.insert(line.strip()) for line in source]\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        self.hdqs[0].insert(sentence + \" _exit\")\n",
    "        print(self.hdqs[0].columns)\n",
    "        print(self.hdqs[1].columns)\n",
    "        print(self.hdqs[2].columns)\n",
    "        print(self.hdqs[3].columns)\n",
    "        print(\"think: \", think(self.hdqs))\n",
    "        \n",
    "    \n",
    "    def treeify(self, tree, level, convo):\n",
    "        if level == 0:\n",
    "            #print(\"returning with \", convo)\n",
    "            tree['word'] = convo\n",
    "            return convo\n",
    "        #print(\"calling self.sdrs[{}][{}:{}]\".format(level-1, convo[0], convo[1]))\n",
    "        next_convos = self.sdrs[level-1][convo[0]:convo[1]]\n",
    "        current_dict = {}\n",
    "        tree[convo[2][0]] = current_dict\n",
    "        for conv in next_convos:\n",
    "            self.treeify(current_dict, level-1, conv)\n",
    "        \n",
    "    def __str__(self):\n",
    "        for sdr in self.sdrs:\n",
    "            print(sdr)\n",
    "            print()\n",
    "        print(self.context)\n",
    "        return \"OK\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'spring': [spring, spring], 'leaves': [leaves], '_exit': [_exit]})\n",
      "defaultdict(<class 'list'>, {'spring': [spring], '0_ADJ': [0_ADJ], '0_VER': [0_VER, 0_VER], '0_NOU': [0_NOU, 0_NOU], 'leaves': [leaves]})\n",
      "defaultdict(<class 'list'>, {'0_ADJ': [0_ADJ], '0_NOU': [0_NOU, 0_NOU], '1_NP': [1_NP], '0_VER': [0_VER], '1_VP': [1_VP]})\n",
      "defaultdict(<class 'list'>, {'1_NP': [1_NP, 1_NP], '1_VP': [1_VP, 1_VP], '2_SENT': [2_SENT, 2_SENT]})\n",
      "think:  [[[[0, 1, ['spring']], [1, 2, ['leaves']], [2, 3, ['spring']]]], [[[0, 1, ['0_ADJ', '0_NOU', '0_VER']], [1, 2, ['0_NOU', '0_VER']], [2, 3, ['0_ADJ', '0_NOU', '0_VER']]]], [[[0, 2, ['1_NP', '1_VP']]], [[1, 3, ['1_VP']]]], []]\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM2(datasource=source_file)\n",
    "\n",
    "lstm.process_sentence(sentence)\n",
    "\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marmalade(d_tree):\n",
    "    sep = 0\n",
    "    for parent_key, child_dict in d_tree.items():\n",
    "        #print('d ', parent_key, child_dict)\n",
    "        if 'word' in child_dict.keys():\n",
    "            print(parent_key, \"-->\", child_dict['word'])\n",
    "            continue\n",
    "        else:\n",
    "            for grand_name, grand_dict in child_dict.items():\n",
    "                print(parent_key, \"-->\", grand_name)\n",
    "            marmalade(child_dict)\n",
    "        sep += 1\n",
    "        \n",
    "for d_tree in lstm.context:\n",
    "    marmalade(d_tree)\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0, 1, ['spring']], [1, 2, ['leaves']], [2, 3, ['spring']]]],\n",
       " [[[0, 1, ['0_ADJ', '0_NOU', '0_VER']],\n",
       "   [1, 2, ['0_NOU', '0_VER']],\n",
       "   [2, 3, ['0_ADJ', '0_NOU', '0_VER']]]],\n",
       " [[[0, 2, ['1_NP', '1_VP']]], [[1, 3, ['1_VP']]]],\n",
       " []]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"spring leaves spring\"\n",
    "\n",
    "hdq1 = Hydraseq('0_')\n",
    "for pattern in [\n",
    "    \"spring 0_ADJ\",\n",
    "    \"spring 0_VER\",\n",
    "    \"spring 0_NOU\",\n",
    "    \"leaves 0_VER\",\n",
    "    \"leaves 0_NOU\",\n",
    "]:\n",
    "    hdq1.insert(pattern)\n",
    "\n",
    "hdq2 = Hydraseq('1_')\n",
    "for pattern in [\n",
    "    \"0_ADJ 0_NOU 1_NP\",\n",
    "    \"0_VER 0_NOU 1_VP\",\n",
    "]:\n",
    "    hdq2.insert(pattern)\n",
    "hdq3 = Hydraseq('2_')\n",
    "for pattern in [\n",
    "    \"1_NP 1_VP 2_SENT\",\n",
    "    \"1_VP 1_NP 2_SENT\",\n",
    "]:\n",
    "    hdq3.insert(pattern)\n",
    "\n",
    "hdq0 = Hydraseq('_')\n",
    "hdq0.insert(sentence + \" _exit\")\n",
    "thoughts = think([hdq0, hdq1, hdq2, hdq3])\n",
    "\n",
    "#assert thoughts[3][0] == [[0, 3, ['2_FACE']]]\n",
    "thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
